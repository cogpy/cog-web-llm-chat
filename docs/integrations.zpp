â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ WebLLM Chat - Integration Contracts Specification (Z++)                     â”‚
â”‚ Formal specification of external service integrations and contracts         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
SECTION 1: WEBLLM API INTEGRATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â”Œâ”€ WebLLMEngineConfig â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Configuration for WebLLM engine initialization                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[WebLLMEngineConfig]
  model: Model
  cacheType: CacheType
  logLevel: {ERROR, WARN, INFO, DEBUG, SILENT}
  temperature: â„
  top_p: â„
  max_tokens: â„•
  
  -- Invariants
  valid_model(model)
  0.0 â‰¤ temperature â‰¤ 2.0
  0.0 â‰¤ top_p â‰¤ 1.0
  0 < max_tokens

â”Œâ”€ InitProgressReport â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Progress report during model initialization                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[InitProgressReport]
  progress: â„
  text: seq char
  timeElapsed: â„•
  
  -- Invariants
  0.0 â‰¤ progress â‰¤ 1.0
  length(text) > 0
  timeElapsed â‰¥ 0

â”Œâ”€ WebLLM_Initialize â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Contract: Initialize WebLLM engine with model                               â”‚
â”‚ External API: ServiceWorkerMLCEngine.reload() or WebWorkerMLCEngine.reload()â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[WebLLM_Initialize]
  config: WebLLMEngineConfig
  onProgress?: InitProgressReport â†’ ()
  
  -- Preconditions
  -- Browser must support WebGPU
  browser_supports_webgpu()
  
  -- Model must be available in prebuilt config or custom
  config.model âˆˆ PREBUILT_MODELS âˆ¨ is_custom_model(config.model)
  
  -- Sufficient storage available
  available_storage() â‰¥ estimated_model_size(config.model)
  
  -- Postconditions (on success)
  model_loaded_in_webgpu(config.model)
  
  -- Callback contract
  onProgress â‰  null âŸ¹
    âˆ€ report: InitProgressReport â€¢ 
      0.0 â‰¤ report.progress â‰¤ 1.0 âˆ§
      final_report.progress = 1.0
  
  -- Error conditions
  Â¬browser_supports_webgpu() âŸ¹ throws(WebGPUNotSupportedError)
  available_storage() < estimated_model_size(config.model) âŸ¹ 
    throws(InsufficientStorageError)
  network_error_during_download() âŸ¹ throws(NetworkError)

â”Œâ”€ WebLLM_Chat â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Contract: Generate chat completion                                          â”‚
â”‚ External API: MLCEngine.chat.completions.create()                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[WebLLM_Chat]
  messages: seq ChatCompletionMessageParam
  stream: ğ”¹
  temperature?: â„
  max_tokens?: â„•
  stop?: seq (seq char)
  onUpdate?: (seq char â†’ ())
  result!: ChatCompletion âˆª Stream(ChatCompletionChunk)
  
  -- Preconditions
  length(messages) > 0
  
  -- All messages have valid roles
  âˆ€ m: ChatCompletionMessageParam â€¢ m âˆˆ messages âŸ¹ 
    m.role âˆˆ {system, user, assistant}
  
  -- Content is non-empty
  âˆ€ m: ChatCompletionMessageParam â€¢ m âˆˆ messages âŸ¹ 
    length(m.content) > 0
  
  -- Model is initialized
  model_loaded_in_webgpu()
  
  -- Streaming requires callback
  stream = true âŸ¹ onUpdate â‰  null
  
  -- Parameter ranges
  temperature â‰  null âŸ¹ 0.0 â‰¤ temperature â‰¤ 2.0
  max_tokens â‰  null âŸ¹ 0 < max_tokens
  
  -- Postconditions (non-streaming)
  stream = false âŸ¹
    result! âˆˆ ChatCompletion âˆ§
    result!.choices[0].message.role = assistant âˆ§
    length(result!.choices[0].message.content) > 0 âˆ§
    result!.usage.total_tokens = 
      result!.usage.prompt_tokens + result!.usage.completion_tokens
  
  -- Postconditions (streaming)
  stream = true âŸ¹
    result! âˆˆ Stream(ChatCompletionChunk) âˆ§
    âˆ€ chunk: ChatCompletionChunk â€¢ chunk âˆˆ result! âŸ¹
      onUpdate(chunk.choices[0].delta.content) âˆ§
      last_chunk.choices[0].finish_reason âˆˆ {stop, length, content_filter}
  
  -- Error conditions
  Â¬model_loaded_in_webgpu() âŸ¹ throws(ModelNotInitializedError)
  context_length_exceeded(messages) âŸ¹ throws(ContextLengthExceededError)
  gpu_out_of_memory() âŸ¹ throws(OutOfMemoryError)

â”Œâ”€ WebLLM_Abort â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Contract: Abort ongoing generation                                          â”‚
â”‚ External API: MLCEngine.interruptGenerate()                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[WebLLM_Abort]
  -- Preconditions
  generation_in_progress()
  
  -- Postconditions
  Â¬generation_in_progress()
  
  -- Streaming callbacks receive no more data
  âˆ€ callback: (seq char â†’ ()) â€¢ no_further_calls(callback)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
SECTION 2: MLC-LLM REST API INTEGRATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â”Œâ”€ MLCLLMEndpoint â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MLC-LLM REST API endpoint configuration                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[MLCLLMEndpoint]
  url: seq char
  apiKey?: seq char
  timeout: â„•
  
  -- Invariants
  is_valid_url(url)
  url starts_with "http://" âˆ¨ url starts_with "https://"
  timeout > 0

â”Œâ”€ MLCLLM_ListModels â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Contract: List available models from MLC-LLM server                         â”‚
â”‚ External API: GET /v1/models                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[MLCLLM_ListModels]
  endpoint: MLCLLMEndpoint
  result!: seq ModelRecord
  
  -- Preconditions
  server_reachable(endpoint.url)
  
  -- HTTP Request
  request_method = GET
  request_url = endpoint.url âŒ¢ "/v1/models"
  request_headers = {
    "Content-Type" â†¦ "application/json",
    "Authorization" â†¦ ("Bearer " âŒ¢ endpoint.apiKey) if endpoint.apiKey â‰  null
  }
  
  -- Postconditions (on success)
  http_status = 200
  length(result!) > 0
  
  âˆ€ m: ModelRecord â€¢ m âˆˆ result! âŸ¹
    length(m.name) > 0 âˆ§
    length(m.display_name) > 0
  
  -- Error conditions
  Â¬server_reachable(endpoint.url) âŸ¹ throws(NetworkError)
  http_status = 401 âŸ¹ throws(AuthenticationError)
  http_status = 404 âŸ¹ throws(EndpointNotFoundError)
  http_status â‰¥ 500 âŸ¹ throws(ServerError)

â”Œâ”€ MLCLLM_CreateCompletion â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Contract: Create chat completion via MLC-LLM REST API                       â”‚
â”‚ External API: POST /v1/chat/completions                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[MLCLLM_CreateCompletion]
  endpoint: MLCLLMEndpoint
  request_body: ChatCompletionRequest
  stream: ğ”¹
  result!: ChatCompletion âˆª Stream(ChatCompletionChunk)
  
  -- Request body schema
  request_body.model: Model
  request_body.messages: seq ChatCompletionMessageParam
  request_body.temperature?: â„
  request_body.top_p?: â„
  request_body.max_tokens?: â„•
  request_body.stream: ğ”¹
  
  -- Preconditions
  server_reachable(endpoint.url)
  length(request_body.messages) > 0
  
  request_body.temperature â‰  null âŸ¹ 0.0 â‰¤ request_body.temperature â‰¤ 2.0
  request_body.top_p â‰  null âŸ¹ 0.0 â‰¤ request_body.top_p â‰¤ 1.0
  request_body.max_tokens â‰  null âŸ¹ 0 < request_body.max_tokens
  
  -- HTTP Request
  request_method = POST
  request_url = endpoint.url âŒ¢ "/v1/chat/completions"
  request_headers = {
    "Content-Type" â†¦ "application/json",
    "Authorization" â†¦ ("Bearer " âŒ¢ endpoint.apiKey) if endpoint.apiKey â‰  null
  }
  request_timeout = endpoint.timeout
  
  -- Postconditions (non-streaming)
  stream = false âŸ¹
    http_status = 200 âˆ§
    result! âˆˆ ChatCompletion âˆ§
    result!.model = request_body.model âˆ§
    result!.choices[0].message.role = assistant âˆ§
    length(result!.choices[0].message.content) > 0
  
  -- Postconditions (streaming)
  stream = true âŸ¹
    http_status = 200 âˆ§
    result! âˆˆ Stream(ChatCompletionChunk) âˆ§
    response_headers["Content-Type"] = "text/event-stream" âˆ§
    âˆ€ chunk: ChatCompletionChunk â€¢ chunk âˆˆ result! âŸ¹
      chunk.model = request_body.model âˆ§
      last_chunk.choices[0].finish_reason â‰  null
  
  -- Error conditions
  Â¬server_reachable(endpoint.url) âŸ¹ throws(NetworkError)
  http_status = 400 âŸ¹ throws(InvalidRequestError)
  http_status = 401 âŸ¹ throws(AuthenticationError)
  http_status = 429 âŸ¹ throws(RateLimitError)
  http_status = 500 âŸ¹ throws(ServerError)
  http_status = 503 âŸ¹ throws(ServiceUnavailableError)
  request_timeout_exceeded() âŸ¹ throws(TimeoutError)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
SECTION 3: COGSERVER INTEGRATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â”Œâ”€ CogServerConnection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ WebSocket connection to CogServer                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[CogServerConnection]
  host: seq char
  port: â„•
  protocol: {ws, tcp}
  reconnect: ğ”¹
  reconnectInterval: â„•
  connected: ğ”¹
  
  -- Invariants
  length(host) > 0
  1 â‰¤ port â‰¤ 65535
  reconnect = true âŸ¹ reconnectInterval > 0
  
  -- Connection URL
  protocol = ws âŸ¹ 
    connection_url = "ws://" âŒ¢ host âŒ¢ ":" âŒ¢ toString(port)

â”Œâ”€ CogServer_Connect â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Contract: Establish WebSocket connection to CogServer                       â”‚
â”‚ External API: WebSocket connection                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[CogServer_Connect]
  config: CogServerConnection
  onConnected?: () â†’ ()
  onError?: (Error â†’ ())
  
  -- Preconditions
  Â¬config.connected
  server_listening(config.host, config.port)
  
  -- Operation
  establish_websocket_connection(config.connection_url)
  
  -- Postconditions (on success)
  config'.connected = true
  onConnected â‰  null âŸ¹ onConnected()
  
  -- Auto-reconnect behavior
  config.reconnect = true âˆ§ connection_lost() âŸ¹
    wait(config.reconnectInterval) âˆ§
    retry_connection()
  
  -- Error conditions
  Â¬server_listening(config.host, config.port) âŸ¹
    config'.connected = false âˆ§
    onError â‰  null âŸ¹ onError(ConnectionRefusedError)
  
  connection_timeout_exceeded() âŸ¹
    config'.connected = false âˆ§
    onError â‰  null âŸ¹ onError(TimeoutError)

â”Œâ”€ CogServer_ExecuteCommand â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Contract: Execute command in CogServer                                      â”‚
â”‚ External API: Send command via WebSocket, receive response                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[CogServer_ExecuteCommand]
  connection: CogServerConnection
  command: seq char
  mode: {atomese, metta, scheme}
  timeout: â„•
  result!: CogServerResponse
  
  -- Preconditions
  connection.connected = true
  length(command) > 0
  timeout > 0
  
  -- Command format validation
  mode = atomese âŸ¹ is_valid_atomese(command)
  mode = metta âŸ¹ is_valid_metta(command)
  mode = scheme âŸ¹ is_valid_scheme(command)
  
  -- Operation
  send_websocket_message(connection, format_command(command, mode))
  
  -- Postconditions (on success)
  result!.success = true
  result!.timestamp = current_time()
  result!.result â‰  null
  
  -- Error handling
  response_timeout_exceeded(timeout) âŸ¹
    result!.success = false âˆ§
    result!.error = "Timeout waiting for response"
  
  invalid_command_syntax() âŸ¹
    result!.success = false âˆ§
    result!.error contains "Syntax error"
  
  server_error() âŸ¹
    result!.success = false âˆ§
    result!.error contains "Server error"

â”Œâ”€ CogServer_QueryAtomSpace â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Contract: Query the AtomSpace via CogServer                                 â”‚
â”‚ External API: Execute query command, parse results                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[CogServer_QueryAtomSpace]
  connection: CogServerConnection
  query: seq char
  result!: seq AtomNode
  
  -- Preconditions
  connection.connected = true
  length(query) > 0
  is_valid_query(query)
  
  -- Operation
  let response = CogServer_ExecuteCommand(
    connection, query, mode := atomese, timeout := 5000)
  
  -- Postconditions (on success)
  response.success = true âŸ¹
    result! = parse_atomspace_response(response.result) âˆ§
    âˆ€ atom: AtomNode â€¢ atom âˆˆ result! âŸ¹ is_well_formed_atom(atom)
  
  -- Error conditions
  response.success = false âŸ¹
    result! = âŸ¨âŸ© âˆ§
    throws(CogServerQueryError(response.error))

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
SECTION 4: BROWSER API INTEGRATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â”Œâ”€ IndexedDB_StoreModel â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Contract: Store model weights in IndexedDB                                  â”‚
â”‚ External API: IndexedDB API                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[IndexedDB_StoreModel]
  modelId: seq char
  weights: seq byte
  metadata: ModelMetadata
  
  -- Preconditions
  length(modelId) > 0
  length(weights) > 0
  browser_supports_indexeddb()
  
  -- Storage quota check
  available_storage() â‰¥ length(weights)
  
  -- Operation
  open_database("webllm-cache")
  create_object_store("models") if Â¬exists
  
  -- Postconditions
  model_exists_in_indexeddb(modelId)
  retrieve_model(modelId).weights = weights
  retrieve_model(modelId).metadata = metadata
  
  -- Error conditions
  Â¬browser_supports_indexeddb() âŸ¹ throws(IndexedDBNotSupportedError)
  available_storage() < length(weights) âŸ¹ throws(QuotaExceededError)
  database_operation_failed() âŸ¹ throws(IndexedDBError)

â”Œâ”€ IndexedDB_RetrieveModel â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Contract: Retrieve model weights from IndexedDB                             â”‚
â”‚ External API: IndexedDB API                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[IndexedDB_RetrieveModel]
  modelId: seq char
  result!: ModelData âˆª null
  
  -- Preconditions
  length(modelId) > 0
  browser_supports_indexeddb()
  
  -- Operation
  open_database("webllm-cache")
  get_from_store("models", modelId)
  
  -- Postconditions
  model_exists_in_indexeddb(modelId) âŸ¹
    result! â‰  null âˆ§
    result!.modelId = modelId âˆ§
    length(result!.weights) > 0
  
  Â¬model_exists_in_indexeddb(modelId) âŸ¹
    result! = null
  
  -- Error conditions
  database_operation_failed() âŸ¹ throws(IndexedDBError)

â”Œâ”€ WebGPU_Initialize â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Contract: Initialize WebGPU adapter and device                              â”‚
â”‚ External API: navigator.gpu.requestAdapter(), adapter.requestDevice()       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[WebGPU_Initialize]
  preferredAdapter?: {powerPreference: {low-power, high-performance}}
  result!: GPUDevice âˆª null
  
  -- Preconditions
  browser_supports_webgpu()
  
  -- Operation
  adapter = navigator.gpu.requestAdapter(preferredAdapter)
  device = adapter.requestDevice()
  
  -- Postconditions (on success)
  adapter â‰  null âˆ§ device â‰  null âŸ¹
    result! = device âˆ§
    device.limits.maxBufferSize > 0 âˆ§
    device.limits.maxComputeWorkgroupSizeX > 0
  
  -- GPU device requirements for LLM inference
  result! â‰  null âŸ¹
    device.limits.maxStorageBufferBindingSize â‰¥ 1GB âˆ§
    device.limits.maxBufferSize â‰¥ 256MB
  
  -- Error conditions
  Â¬browser_supports_webgpu() âŸ¹
    result! = null âˆ§
    throws(WebGPUNotSupportedError)
  
  adapter = null âŸ¹
    result! = null âˆ§
    throws(NoWebGPUAdapterError)

â”Œâ”€ LocalStorage_PersistState â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Contract: Persist application state to LocalStorage                         â”‚
â”‚ External API: localStorage.setItem()                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[LocalStorage_PersistState]
  key: seq char
  state: SystemState
  
  -- Preconditions
  length(key) > 0
  browser_supports_localstorage()
  
  -- Serialization
  let serialized = JSON.stringify(state)
  
  -- Size check
  length(serialized) â‰¤ LOCALSTORAGE_QUOTA
  
  -- Operation
  localStorage.setItem(key, serialized)
  
  -- Postconditions
  localStorage.getItem(key) = serialized
  JSON.parse(localStorage.getItem(key)) = state
  
  -- Error conditions
  Â¬browser_supports_localstorage() âŸ¹ throws(LocalStorageNotSupportedError)
  length(serialized) > LOCALSTORAGE_QUOTA âŸ¹ throws(QuotaExceededError)

â”Œâ”€ ServiceWorker_Register â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Contract: Register service worker for PWA functionality                     â”‚
â”‚ External API: navigator.serviceWorker.register()                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[ServiceWorker_Register]
  scriptURL: seq char
  scope?: seq char
  result!: ServiceWorkerRegistration âˆª null
  
  -- Preconditions
  browser_supports_serviceworker()
  is_secure_context()  -- HTTPS or localhost
  is_valid_url(scriptURL)
  
  -- Operation
  registration = navigator.serviceWorker.register(scriptURL, {scope})
  
  -- Postconditions (on success)
  registration â‰  null âŸ¹
    result! = registration âˆ§
    registration.active â‰  null âˆ¨ registration.installing â‰  null
  
  -- Service worker lifecycle
  registration.installing â‰  null âŸ¹
    eventually(registration.active â‰  null)
  
  -- Error conditions
  Â¬browser_supports_serviceworker() âŸ¹
    result! = null âˆ§
    throws(ServiceWorkerNotSupportedError)
  
  Â¬is_secure_context() âŸ¹
    result! = null âˆ§
    throws(SecurityError("Service workers require HTTPS"))
  
  invalid_script_url() âŸ¹
    result! = null âˆ§
    throws(NetworkError)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
SECTION 5: INTEGRATION INVARIANTS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â”Œâ”€ SystemIntegrationInvariants â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Cross-cutting invariants across all integrations                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[SystemIntegrationInvariants]
  systemState: SystemState
  
  -- WebLLM and MLC-LLM are mutually exclusive
  mutual_exclusion:
    Â¬(systemState.webllmState.isGenerating âˆ§ 
      systemState.mlcllmState.isGenerating)
  
  -- Client type consistency
  client_consistency:
    systemState.configStore.config.modelClientType = WEBLLM âŸ¹
      systemState.webllmState.initialized = true âˆ§
    systemState.configStore.config.modelClientType = MLCLLM_API âŸ¹
      systemState.mlcllmState.connected = true
  
  -- Generation source consistency
  generation_source:
    systemState.chatStore.sessions[systemState.chatStore.currentSessionIndex]
      .isGenerating âŸ¹
      (systemState.webllmState.isGenerating âˆ¨ 
       systemState.mlcllmState.isGenerating)
  
  -- CogServer optional integration
  cogserver_independence:
    âˆ€ op: AgentOperation â€¢ 
      op requires_cogserver() âŸ¹
        systemState.orchestrationState.cogServer.connected = true âˆ¨
        use_mock_cogserver()
  
  -- Browser API availability
  browser_compatibility:
    systemState.configStore.config.modelClientType = WEBLLM âŸ¹
      browser_supports_webgpu() âˆ§
      browser_supports_indexeddb() âˆ§
      browser_supports_serviceworker()
  
  -- Storage consistency
  storage_integrity:
    âˆ€ model: Model â€¢ model âˆˆ systemState.webllmState.modelCache âŸ¹
      (model_exists_in_indexeddb(model) âˆ¨ 
       model_in_memory(model))
  
  -- State persistence consistency
  persistence_consistency:
    systemState persisted_in_localstorage() âŸ¹
      JSON.parse(localStorage.getItem(STORAGE_KEY)) = systemState

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
SECTION 6: ERROR HANDLING CONTRACTS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â”Œâ”€ ErrorRecoveryStrategy â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Standard error recovery patterns across integrations                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[ErrorRecoveryStrategy]
  
  -- Network errors: retry with exponential backoff
  network_error_recovery:
    âˆ€ err: NetworkError â€¢ 
      retry_count < MAX_RETRIES âŸ¹
        wait(BACKOFF_DELAY Ã— 2^retry_count) âˆ§
        retry_operation() âˆ§
        retry_count' = retry_count + 1
  
  -- GPU errors: fallback to different model or reduce precision
  gpu_error_recovery:
    âˆ€ err: OutOfMemoryError â€¢
      try_smaller_model() âˆ¨
      reduce_context_window() âˆ¨
      notify_user("Insufficient GPU memory")
  
  -- Storage errors: use alternative storage or in-memory
  storage_error_recovery:
    âˆ€ err: QuotaExceededError â€¢
      clear_old_cached_models() âˆ¨
      use_cache_mode() âˆ¨
      notify_user("Storage quota exceeded")
  
  -- CogServer errors: use mock or disable feature
  cogserver_error_recovery:
    âˆ€ err: ConnectionRefusedError â€¢
      use_mock_cogserver() âˆ¨
      disable_opencog_features() âˆ¨
      notify_user("CogServer unavailable")

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
SECTION 7: HELPER FUNCTIONS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

[Helper Functions]
  browser_supports_webgpu: â†’ ğ”¹
  browser_supports_indexeddb: â†’ ğ”¹
  browser_supports_serviceworker: â†’ ğ”¹
  browser_supports_localstorage: â†’ ğ”¹
  is_secure_context: â†’ ğ”¹
  is_valid_url: seq char â†’ ğ”¹
  is_valid_atomese: seq char â†’ ğ”¹
  is_valid_metta: seq char â†’ ğ”¹
  is_valid_scheme: seq char â†’ ğ”¹
  is_valid_query: seq char â†’ ğ”¹
  is_well_formed_atom: AtomNode â†’ ğ”¹
  server_reachable: seq char â†’ ğ”¹
  server_listening: seq char Ã— â„• â†’ ğ”¹
  available_storage: â†’ â„•
  estimated_model_size: Model â†’ â„•
  model_loaded_in_webgpu: Model â†’ ğ”¹
  model_exists_in_indexeddb: seq char â†’ ğ”¹
  model_in_memory: Model â†’ ğ”¹
  generation_in_progress: â†’ ğ”¹
  context_length_exceeded: seq ChatCompletionMessageParam â†’ ğ”¹
  gpu_out_of_memory: â†’ ğ”¹
  parse_atomspace_response: any â†’ seq AtomNode
  format_command: seq char Ã— {atomese, metta, scheme} â†’ seq char

[Constants]
  PREBUILT_MODELS: â„™ Model
  MAX_RETRIES: â„•
  BACKOFF_DELAY: â„•
  LOCALSTORAGE_QUOTA: â„•
  STORAGE_KEY: seq char

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
END OF INTEGRATION CONTRACTS SPECIFICATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
